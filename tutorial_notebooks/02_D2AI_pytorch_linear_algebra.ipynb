{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andrian0s/ML4NLP1-2023-Tutorial-Notebooks/blob/main/tutorial_notebooks/02_D2AI_pytorch_linear_algebra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a622c08e",
      "metadata": {
        "origin_pos": 1,
        "id": "a622c08e"
      },
      "source": [
        "# Linear Algebra\n",
        ":label:`sec_linear-algebra`\n",
        "\n",
        "By now, we can load datasets into tensors\n",
        "and manipulate these tensors\n",
        "with basic mathematical operations.\n",
        "To start building sophisticated models,\n",
        "we will also need a few tools from linear algebra.\n",
        "This section offers a gentle introduction\n",
        "to the most essential concepts,\n",
        "starting from scalar arithmetic\n",
        "and ramping up to matrix multiplication.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc36b473",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:41:51.529162Z",
          "iopub.status.busy": "2023-08-18T19:41:51.528467Z",
          "iopub.status.idle": "2023-08-18T19:41:53.438267Z",
          "shell.execute_reply": "2023-08-18T19:41:53.437059Z"
        },
        "origin_pos": 3,
        "tab": [
          "pytorch"
        ],
        "id": "dc36b473"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4a38674",
      "metadata": {
        "origin_pos": 6,
        "id": "e4a38674"
      },
      "source": [
        "## Scalars\n",
        "\n",
        "\n",
        "Most everyday mathematics\n",
        "consists of manipulating\n",
        "numbers one at a time.\n",
        "Formally, we call these values *scalars*.\n",
        "For example, the temperature in Palo Alto\n",
        "is a balmy $72$ degrees Fahrenheit.\n",
        "If you wanted to convert the temperature to Celsius\n",
        "you would evaluate the expression\n",
        "$c = \\frac{5}{9}(f - 32)$, setting $f$ to $72$.\n",
        "In this equation, the values\n",
        "$5$, $9$, and $32$ are constant scalars.\n",
        "The variables $c$ and $f$\n",
        "in general represent unknown scalars.\n",
        "\n",
        "We denote scalars\n",
        "by ordinary lower-cased letters\n",
        "(e.g., $x$, $y$, and $z$)\n",
        "and the space of all (continuous)\n",
        "*real-valued* scalars by $\\mathbb{R}$.\n",
        "For expedience, we will skip past\n",
        "rigorous definitions of *spaces*:\n",
        "just remember that the expression $x \\in \\mathbb{R}$\n",
        "is a formal way to say that $x$ is a real-valued scalar.\n",
        "The symbol $\\in$ (pronounced \"in\")\n",
        "denotes membership in a set.\n",
        "For example, $x, y \\in \\{0, 1\\}$\n",
        "indicates that $x$ and $y$ are variables\n",
        "that can only take values $0$ or $1$.\n",
        "\n",
        "(**Scalars are implemented as tensors\n",
        "that contain only one element.**)\n",
        "Below, we assign two scalars\n",
        "and perform the familiar addition, multiplication,\n",
        "division, and exponentiation operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fc9ba1d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:41:53.442690Z",
          "iopub.status.busy": "2023-08-18T19:41:53.442040Z",
          "iopub.status.idle": "2023-08-18T19:41:53.472277Z",
          "shell.execute_reply": "2023-08-18T19:41:53.471491Z"
        },
        "origin_pos": 8,
        "tab": [
          "pytorch"
        ],
        "id": "4fc9ba1d",
        "outputId": "d3b53ca0-819a-4119-89fc-f40c62c17b62"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.tensor(3.0)\n",
        "y = torch.tensor(2.0)\n",
        "\n",
        "x + y, x * y, x / y, x**y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b20517b",
      "metadata": {
        "origin_pos": 11,
        "id": "0b20517b"
      },
      "source": [
        "## Vectors\n",
        "\n",
        "For current purposes, [**you can think of a vector as a fixed-length array of scalars.**]\n",
        "As with their code counterparts,\n",
        "we call these scalars the *elements* of the vector\n",
        "(synonyms include *entries* and *components*).\n",
        "When vectors represent examples from real-world datasets,\n",
        "their values hold some real-world significance.\n",
        "For example, if we were training a model to predict\n",
        "the risk of a loan defaulting,\n",
        "we might associate each applicant with a vector\n",
        "whose components correspond to quantities\n",
        "like their income, length of employment,\n",
        "or number of previous defaults.\n",
        "If we were studying the risk of heart attack,\n",
        "each vector might represent a patient\n",
        "and its components might correspond to\n",
        "their most recent vital signs, cholesterol levels,\n",
        "minutes of exercise per day, etc.\n",
        "We denote vectors by bold lowercase letters,\n",
        "(e.g., $\\mathbf{x}$, $\\mathbf{y}$, and $\\mathbf{z}$).\n",
        "\n",
        "Vectors are implemented as $1^{\\textrm{st}}$-order tensors.\n",
        "In general, such tensors can have arbitrary lengths,\n",
        "subject to memory limitations. Caution: in Python, as in most programming languages, vector indices start at $0$, also known as *zero-based indexing*, whereas in linear algebra subscripts begin at $1$ (one-based indexing).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91cd966f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:41:53.475759Z",
          "iopub.status.busy": "2023-08-18T19:41:53.475141Z",
          "iopub.status.idle": "2023-08-18T19:41:53.481106Z",
          "shell.execute_reply": "2023-08-18T19:41:53.479872Z"
        },
        "origin_pos": 13,
        "tab": [
          "pytorch"
        ],
        "id": "91cd966f",
        "outputId": "36e18c7f-5edb-49ff-e6f9-3cb183be6a0b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0, 1, 2])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.arange(3)\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a603956",
      "metadata": {
        "origin_pos": 16,
        "id": "7a603956"
      },
      "source": [
        "We can refer to an element of a vector by using a subscript.\n",
        "For example, $x_2$ denotes the second element of $\\mathbf{x}$.\n",
        "Since $x_2$ is a scalar, we do not bold it.\n",
        "By default, we visualize vectors\n",
        "by stacking their elements vertically.\n",
        "\n",
        "$$\\mathbf{x} =\\begin{bmatrix}x_{1}  \\\\ \\vdots  \\\\x_{n}\\end{bmatrix},$$\n",
        ":eqlabel:`eq_vec_def`\n",
        "\n",
        "Here $x_1, \\ldots, x_n$ are elements of the vector.\n",
        "Later on, we will distinguish between such *column vectors*\n",
        "and *row vectors* whose elements are stacked horizontally.\n",
        "Recall that [**we access a tensor's elements via indexing.**]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba15a197",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:41:53.485066Z",
          "iopub.status.busy": "2023-08-18T19:41:53.484260Z",
          "iopub.status.idle": "2023-08-18T19:41:53.492710Z",
          "shell.execute_reply": "2023-08-18T19:41:53.491415Z"
        },
        "origin_pos": 17,
        "tab": [
          "pytorch"
        ],
        "id": "ba15a197",
        "outputId": "8c03b7cc-037d-456e-e737-4370f098e182"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(2)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02a04823",
      "metadata": {
        "origin_pos": 18,
        "id": "02a04823"
      },
      "source": [
        "To indicate that a vector contains $n$ elements,\n",
        "we write $\\mathbf{x} \\in \\mathbb{R}^n$.\n",
        "Formally, we call $n$ the *dimensionality* of the vector.\n",
        "[**In code, this corresponds to the tensor's length**],\n",
        "accessible via Python's built-in `len` function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "871cd7e6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:41:53.497529Z",
          "iopub.status.busy": "2023-08-18T19:41:53.496794Z",
          "iopub.status.idle": "2023-08-18T19:41:53.502332Z",
          "shell.execute_reply": "2023-08-18T19:41:53.501510Z"
        },
        "origin_pos": 19,
        "tab": [
          "pytorch"
        ],
        "id": "871cd7e6",
        "outputId": "d6b5220f-738f-43cd-c6bf-8c3f2e5b0f34"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e190d69",
      "metadata": {
        "origin_pos": 20,
        "id": "4e190d69"
      },
      "source": [
        "We can also access the length via the `shape` attribute.\n",
        "The shape is a tuple that indicates a tensor's length along each axis.\n",
        "(**Tensors with just one axis have shapes with just one element.**)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34ea04c3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:41:53.505748Z",
          "iopub.status.busy": "2023-08-18T19:41:53.505180Z",
          "iopub.status.idle": "2023-08-18T19:41:53.510136Z",
          "shell.execute_reply": "2023-08-18T19:41:53.509337Z"
        },
        "origin_pos": 21,
        "tab": [
          "pytorch"
        ],
        "id": "34ea04c3",
        "outputId": "ef7189ca-ec29-45d5-e3cc-6bda858062ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e416733f",
      "metadata": {
        "origin_pos": 22,
        "id": "e416733f"
      },
      "source": [
        "Oftentimes, the word \"dimension\" gets overloaded\n",
        "to mean both the number of axes\n",
        "and the length along a particular axis.\n",
        "To avoid this confusion,\n",
        "we use *order* to refer to the number of axes\n",
        "and *dimensionality* exclusively to refer\n",
        "to the number of components.\n",
        "\n",
        "\n",
        "## Matrices\n",
        "\n",
        "Just as scalars are $0^{\\textrm{th}}$-order tensors\n",
        "and vectors are $1^{\\textrm{st}}$-order tensors,\n",
        "matrices are $2^{\\textrm{nd}}$-order tensors.\n",
        "We denote matrices by bold capital letters\n",
        "(e.g., $\\mathbf{X}$, $\\mathbf{Y}$, and $\\mathbf{Z}$),\n",
        "and represent them in code by tensors with two axes.\n",
        "The expression $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$\n",
        "indicates that a matrix $\\mathbf{A}$\n",
        "contains $m \\times n$ real-valued scalars,\n",
        "arranged as $m$ rows and $n$ columns.\n",
        "When $m = n$, we say that a matrix is *square*.\n",
        "Visually, we can illustrate any matrix as a table.\n",
        "To refer to an individual element,\n",
        "we subscript both the row and column indices, e.g.,\n",
        "$a_{ij}$ is the value that belongs to $\\mathbf{A}$'s\n",
        "$i^{\\textrm{th}}$ row and $j^{\\textrm{th}}$ column:\n",
        "\n",
        "$$\\mathbf{A}=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix}.$$\n",
        ":eqlabel:`eq_matrix_def`\n",
        "\n",
        "\n",
        "In code, we represent a matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$\n",
        "by a $2^{\\textrm{nd}}$-order tensor with shape ($m$, $n$).\n",
        "[**We can convert any appropriately sized $m \\times n$ tensor\n",
        "into an $m \\times n$ matrix**]\n",
        "by passing the desired shape to `reshape`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80c49751",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:41:53.513569Z",
          "iopub.status.busy": "2023-08-18T19:41:53.512931Z",
          "iopub.status.idle": "2023-08-18T19:41:53.518545Z",
          "shell.execute_reply": "2023-08-18T19:41:53.517740Z"
        },
        "origin_pos": 24,
        "tab": [
          "pytorch"
        ],
        "id": "80c49751",
        "outputId": "49982eca-9e48-40fc-d8e7-5f1ad4bf6241"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0, 1],\n",
              "        [2, 3],\n",
              "        [4, 5]])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "A = torch.arange(6).reshape(3, 2)\n",
        "A"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d90da51",
      "metadata": {
        "origin_pos": 27,
        "id": "0d90da51"
      },
      "source": [
        "Sometimes we want to flip the axes.\n",
        "When we exchange a matrix's rows and columns,\n",
        "the result is called its *transpose*.\n",
        "Formally, we signify a matrix $\\mathbf{A}$'s transpose\n",
        "by $\\mathbf{A}^\\top$ and if $\\mathbf{B} = \\mathbf{A}^\\top$,\n",
        "then $b_{ij} = a_{ji}$ for all $i$ and $j$.\n",
        "Thus, the transpose of an $m \\times n$ matrix\n",
        "is an $n \\times m$ matrix:\n",
        "\n",
        "$$\n",
        "\\mathbf{A}^\\top =\n",
        "\\begin{bmatrix}\n",
        "    a_{11} & a_{21} & \\dots  & a_{m1} \\\\\n",
        "    a_{12} & a_{22} & \\dots  & a_{m2} \\\\\n",
        "    \\vdots & \\vdots & \\ddots  & \\vdots \\\\\n",
        "    a_{1n} & a_{2n} & \\dots  & a_{mn}\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "In code, we can access any (**matrix's transpose**) as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ef1e23b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:41:53.521874Z",
          "iopub.status.busy": "2023-08-18T19:41:53.521332Z",
          "iopub.status.idle": "2023-08-18T19:41:53.526566Z",
          "shell.execute_reply": "2023-08-18T19:41:53.525812Z"
        },
        "origin_pos": 28,
        "tab": [
          "pytorch"
        ],
        "id": "7ef1e23b",
        "outputId": "1c8111a6-8c87-4abe-9608-c022a0f00681"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0, 2, 4],\n",
              "        [1, 3, 5]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "A.T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce337753",
      "metadata": {
        "origin_pos": 30,
        "id": "ce337753"
      },
      "source": [
        "[**Symmetric matrices are the subset of square matrices\n",
        "that are equal to their own transposes:\n",
        "$\\mathbf{A} = \\mathbf{A}^\\top$.**]\n",
        "The following matrix is symmetric:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "028e06ed",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:41:53.529939Z",
          "iopub.status.busy": "2023-08-18T19:41:53.529400Z",
          "iopub.status.idle": "2023-08-18T19:41:53.535337Z",
          "shell.execute_reply": "2023-08-18T19:41:53.534568Z"
        },
        "origin_pos": 32,
        "tab": [
          "pytorch"
        ],
        "id": "028e06ed",
        "outputId": "da8e5b40-f229-442c-e448-29901fe3f0ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[True, True, True],\n",
              "        [True, True, True],\n",
              "        [True, True, True]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "A = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
        "A == A.T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f945d82",
      "metadata": {
        "origin_pos": 35,
        "id": "2f945d82"
      },
      "source": [
        "Matrices are useful for representing datasets.\n",
        "Typically, rows correspond to individual records\n",
        "and columns correspond to distinct attributes.\n",
        "\n",
        "\n",
        "\n",
        "## Tensors\n",
        "\n",
        "While you can go far in your machine learning journey\n",
        "with only scalars, vectors, and matrices,\n",
        "eventually you may need to work with\n",
        "higher-order [**tensors**].\n",
        "Tensors (**give us a generic way of describing\n",
        "extensions to $n^{\\textrm{th}}$-order arrays.**)\n",
        "We call software objects of the *tensor class* \"tensors\"\n",
        "precisely because they too can have arbitrary numbers of axes.\n",
        "While it may be confusing to use the word\n",
        "*tensor* for both the mathematical object\n",
        "and its realization in code,\n",
        "our meaning should usually be clear from context.\n",
        "We denote general tensors by capital letters\n",
        "with a special font face\n",
        "(e.g., $\\mathsf{X}$, $\\mathsf{Y}$, and $\\mathsf{Z}$)\n",
        "and their indexing mechanism\n",
        "(e.g., $x_{ijk}$ and $[\\mathsf{X}]_{1, 2i-1, 3}$)\n",
        "follows naturally from that of matrices.\n",
        "\n",
        "Tensors will become more important\n",
        "when we start working with images.\n",
        "Each image arrives as a $3^{\\textrm{rd}}$-order tensor\n",
        "with axes corresponding to the height, width, and *channel*.\n",
        "At each spatial location, the intensities\n",
        "of each color (red, green, and blue)\n",
        "are stacked along the channel.\n",
        "Furthermore, a collection of images is represented\n",
        "in code by a $4^{\\textrm{th}}$-order tensor,\n",
        "where distinct images are indexed\n",
        "along the first axis.\n",
        "Higher-order tensors are constructed, as were vectors and matrices,\n",
        "by growing the number of shape components.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "306d610e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:41:53.538891Z",
          "iopub.status.busy": "2023-08-18T19:41:53.538210Z",
          "iopub.status.idle": "2023-08-18T19:41:53.546164Z",
          "shell.execute_reply": "2023-08-18T19:41:53.545027Z"
        },
        "origin_pos": 37,
        "tab": [
          "pytorch"
        ],
        "id": "306d610e",
        "outputId": "c8cebf03-8934-4e13-be56-f3c8628e9912"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 0,  1,  2,  3],\n",
              "         [ 4,  5,  6,  7],\n",
              "         [ 8,  9, 10, 11]],\n",
              "\n",
              "        [[12, 13, 14, 15],\n",
              "         [16, 17, 18, 19],\n",
              "         [20, 21, 22, 23]]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.arange(24).reshape(2, 3, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3113bae4",
      "metadata": {
        "origin_pos": 40,
        "id": "3113bae4"
      },
      "source": [
        "## Basic Properties of Tensor Arithmetic\n",
        "\n",
        "Scalars, vectors, matrices,\n",
        "and higher-order tensors\n",
        "all have some handy properties.\n",
        "For example, elementwise operations\n",
        "produce outputs that have the\n",
        "same shape as their operands.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53a34bc0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:41:53.550917Z",
          "iopub.status.busy": "2023-08-18T19:41:53.550017Z",
          "iopub.status.idle": "2023-08-18T19:41:53.558241Z",
          "shell.execute_reply": "2023-08-18T19:41:53.557366Z"
        },
        "origin_pos": 42,
        "tab": [
          "pytorch"
        ],
        "id": "53a34bc0",
        "outputId": "14794253-5d95-4f6e-faef-c3952b3f755e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[0., 1., 2.],\n",
              "         [3., 4., 5.]]),\n",
              " tensor([[ 0.,  2.,  4.],\n",
              "         [ 6.,  8., 10.]]))"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
        "B = A.clone()  # Assign a copy of A to B by allocating new memory\n",
        "A, A + B"
      ]
    },
    {
      