{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andrian0s/ML4NLP1-2023-Tutorial-Notebooks/blob/main/tutorial_notebooks/07_tutorial_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7Ic2n5f6Wve"
      },
      "outputs": [],
      "source": [
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyrhwMu0BoJZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TITzFYHoOKeT"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data as data, torchvision as tv\n",
        "import lightning as L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNNEe9D1CvNW"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.blank('en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8I3RZErWOKeU",
        "outputId": "6cefa911-18f2-4967-9056-66a332990abd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 7 GPU(s) available.\n",
            "Device name: NVIDIA GeForce GTX TITAN X\n"
          ]
        }
      ],
      "source": [
        "# use the GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZY5WvZMPBuMZ"
      },
      "outputs": [],
      "source": [
        "# download the dataset with wget\n",
        "# if the dataset is on github, try git clone instead.\n",
        "!wget -P \"data/\" https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\n",
        "# unpack the file\n",
        "!tar xvzf 'data/rt-polaritydata.tar.gz' -C 'data/'\n",
        "!mv data/rt-polaritydata/rt-polarity.neg data/\n",
        "!mv data/rt-polaritydata/rt-polarity.pos data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZV7yzsYDk5D"
      },
      "outputs": [],
      "source": [
        "# import the dataset (txt file) line by line\n",
        "def load_text(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        texts = []\n",
        "        for line in f:\n",
        "            texts.append(line.decode(errors='ignore').lower().strip())\n",
        "    return texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxRgXT3vD2ix"
      },
      "outputs": [],
      "source": [
        "neg_text = load_text(\"movie_review_data/data/rt-polarity.neg\")\n",
        "pos_text = load_text(\"movie_review_data/data/rt-polarity.pos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vI5fdo22UKqe"
      },
      "outputs": [],
      "source": [
        "# concat negative and positive texts\n",
        "texts = neg_text + pos_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64j9QLGHOKeW",
        "outputId": "bb57b346-392f-4657-daa2-ae0fbf114a44"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['simplistic , silly and tedious .',\n",
              " \"it's so laddish and juvenile , only teenage boys could possibly find it funny .\",\n",
              " 'exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable .',\n",
              " '[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation .',\n",
              " 'a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification .']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6tsNU7gUgoF"
      },
      "outputs": [],
      "source": [
        "# we know the order in texts variable, so we can label it accordingly\n",
        "labels = np.array([0]*len(neg_text) + [1]*len(pos_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqP3wWAbffH_"
      },
      "outputs": [],
      "source": [
        "def tokenize(texts):\n",
        "  \"\"\"\n",
        "  Assign unique id to each token\n",
        "  \"\"\"\n",
        "  max_len = 0\n",
        "  tokenized_texts = []\n",
        "  word2idx = {}\n",
        "\n",
        "  # Add <pad> and <unk> tokens to the vocabulary\n",
        "  word2idx['<pad>'] = 0\n",
        "  word2idx['<unk>'] = 1\n",
        "\n",
        "  # Building our vocab from the corpus starting from index 2\n",
        "  idx = 2\n",
        "  for sent in texts:\n",
        "    tokenized_sent = nlp(sent)\n",
        "    # Add `tokenized_sent` to `tokenized_texts`\n",
        "    tokenized_texts.append(tokenized_sent)\n",
        "    # Add new token to `word2idx`\n",
        "    for token in tokenized_sent:\n",
        "      # string any token objects are different things, be careful.\n",
        "      if token.text not in word2idx:\n",
        "        word2idx[token.text] = idx\n",
        "        idx += 1\n",
        "\n",
        "        # Update `max_len`\n",
        "    max_len = max(max_len, len(tokenized_sent))\n",
        "\n",
        "  return tokenized_texts, word2idx, max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2N0qAlGJicxG"
      },
      "outputs": [],
      "source": [
        "def encode(tokenized_texts, word2idx, max_len):\n",
        "    input_ids = []\n",
        "    for tokenized_sent in tokenized_texts:\n",
        "        # Pad sentences to max_len\n",
        "        tokenized_padded_sent = list(tokenized_sent) + ['<pad>'] * (max_len - len(tokenized_sent))\n",
        "\n",
        "        # Encode tokens to input_ids\n",
        "        input_id = [word2idx.get(str(token)) for token in tokenized_padded_sent]\n",
        "        input_ids.append(input_id)\n",
        "\n",
        "    return np.array(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0GvkEDscK41"
      },
      "outputs": [],
      "source": [
        "tokenized_texts, word2idx, max_len = tokenize(texts)\n",
        "input_ids = encode(tokenized_texts, word2idx, max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
    