{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andrian0s/ML4NLP1-2023-Tutorial-Notebooks/blob/main/tutorial_notebooks/06_similarity_analogy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4e27f41",
      "metadata": {
        "id": "f4e27f41"
      },
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook. Note that running on Colab is experimental, please report a Github\n",
        "issue if you have any problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "455293d9",
      "metadata": {
        "id": "455293d9"
      },
      "outputs": [],
      "source": [
        "!pip install d2l==1.0.3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9bc39df",
      "metadata": {
        "origin_pos": 0,
        "id": "d9bc39df"
      },
      "source": [
        "# Word Similarity and Analogy\n",
        ":label:`sec_synonyms`\n",
        "\n",
        "In :numref:`sec_word2vec_pretraining`,\n",
        "we trained a word2vec model on a small dataset,\n",
        "and applied it\n",
        "to find semantically similar words\n",
        "for an input word.\n",
        "In practice,\n",
        "word vectors that are pretrained\n",
        "on large corpora can be\n",
        "applied to downstream\n",
        "natural language processing tasks,\n",
        "which will be covered later\n",
        "in :numref:`chap_nlp_app`.\n",
        "To demonstrate\n",
        "semantics of pretrained word vectors\n",
        "from large corpora in a straightforward way,\n",
        "let's apply them\n",
        "in the word similarity and analogy tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c36748f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:39:58.110484Z",
          "iopub.status.busy": "2023-08-18T19:39:58.109802Z",
          "iopub.status.idle": "2023-08-18T19:40:01.601183Z",
          "shell.execute_reply": "2023-08-18T19:40:01.599880Z"
        },
        "origin_pos": 2,
        "tab": [
          "pytorch"
        ],
        "id": "7c36748f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58dea0ce",
      "metadata": {
        "origin_pos": 3,
        "id": "58dea0ce"
      },
      "source": [
        "## Loading Pretrained Word Vectors\n",
        "\n",
        "Below lists pretrained GloVe embeddings of dimension 50, 100, and 300,\n",
        "which can be downloaded from the [GloVe website](https://nlp.stanford.edu/projects/glove/).\n",
        "The pretrained fastText embeddings are available in multiple languages.\n",
        "Here we consider one English version (300-dimensional \"wiki.en\") that can be downloaded from the\n",
        "[fastText website](https://fasttext.cc/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "740b8826",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:40:01.605791Z",
          "iopub.status.busy": "2023-08-18T19:40:01.605008Z",
          "iopub.status.idle": "2023-08-18T19:40:01.610925Z",
          "shell.execute_reply": "2023-08-18T19:40:01.609999Z"
        },
        "origin_pos": 4,
        "tab": [
          "pytorch"
        ],
        "id": "740b8826"
      },
      "outputs": [],
      "source": [
        "#@save\n",
        "d2l.DATA_HUB['glove.6b.50d'] = (d2l.DATA_URL + 'glove.6B.50d.zip',\n",
        "                                '0b8703943ccdb6eb788e6f091b8946e82231bc4d')\n",
        "\n",
        "#@save\n",
        "d2l.DATA_HUB['glove.6b.100d'] = (d2l.DATA_URL + 'glove.6B.100d.zip',\n",
        "                                 'cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a')\n",
        "\n",
        "#@save\n",
        "d2l.DATA_HUB['glove.42b.300d'] = (d2l.DATA_URL + 'glove.42B.300d.zip',\n",
        "                                  'b5116e234e9eb9076672cfeabf5469f3eec904fa')\n",
        "\n",
        "#@save\n",
        "d2l.DATA_HUB['wiki.en'] = (d2l.DATA_URL + 'wiki.en.zip',\n",
        "                           'c1816da3821ae9f43899be655002f6c723e91b88')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6e66b37",
      "metadata": {
        "origin_pos": 5,
        "id": "c6e66b37"
      },
      "source": [
        "To load these pretrained GloVe and fastText embeddings, we define the following `TokenEmbedding` class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45117bfd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:40:01.615965Z",
          "iopub.status.busy": "2023-08-18T19:40:01.614818Z",
          "iopub.status.idle": "2023-08-18T19:40:01.625449Z",
          "shell.execute_reply": "2023-08-18T19:40:01.624404Z"
        },
        "origin_pos": 6,
        "tab": [
          "pytorch"
        ],
        "id": "45117bfd"
      },
      "outputs": [],
      "source": [
        "#@save\n",
        "class TokenEmbedding:\n",
        "    \"\"\"Token Embedding.\"\"\"\n",
        "    def __init__(self, embedding_name):\n",
        "        self.idx_to_token, self.idx_to_vec = self._load_embedding(\n",
        "            embedding_name)\n",
        "        self.unknown_idx = 0\n",
        "        self.token_to_idx = {token: idx for idx, token in\n",
        "                             enumerate(self.idx_to_token)}\n",
        "\n",
        "    def _load_embedding(self, embedding_name):\n",
        "        idx_to_token, idx_to_vec = ['<unk>'], []\n",
        "        data_dir = d2l.download_extract(embedding_name)\n",
        "        # GloVe website: https://nlp.stanford.edu/projects/glove/\n",
        "        # fastText website: https://fasttext.cc/\n",
        "        with open(os.path.join(data_dir, 'vec.txt'), 'r') as f:\n",
        "            for line in f:\n",
        "                elems = line.rstrip().split(' ')\n",
        "                token, elems = elems[0], [float(elem) for elem in elems[1:]]\n",
        "                # Skip header information, such as the top row in fastText\n",
        "                if len(elems) > 1:\n",
        "                    idx_to_token.append(token)\n",
        "                    idx_to_vec.append(elems)\n",
        "        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\n",
        "        return idx_to_token, torch.tensor(idx_to_vec)\n",
        "\n",
        "    def __getitem__(self, tokens):\n",
        "        indices = [self.token_to_idx.get(token, self.unknown_idx)\n",
        "                   for token in tokens]\n",
        "        vecs = self.idx_to_vec[torch.tensor(indices)]\n",
        "        return vecs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "927d092b",
      "metadata": {
        "origin_pos": 7,
        "id": "927d092b"
      },
      "source": [
        "Below we load the\n",
        "50-dimensional GloVe embeddings\n",
        "(pretrained on a Wikipedia subset).\n",
        "When creating the `TokenEmbedding` instance,\n",
        "the specified embedding file has to be downloaded if it\n",
        "was not yet.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4487556e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:40:01.630355Z",
          "iopub.status.busy": "2023-08-18T19:40:01.630031Z",
          "iopub.status.idle": "2023-08-18T19:40:18.097173Z",
          "shell.execute_reply": "2023-08-18T19:40:18.096139Z"
        },
        "origin_pos": 8,
        "tab": [
          "pytorch"
        ],
        "id": "4487556e",
        "outputId": "f94b10d7-c742-45d4-d1e7-860fab124e91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading ../data/glove.6B.50d.zip from http://d2l-data.s3-accelerate.amazonaws.com/glove.6B.50d.zip...\n"
          ]
        }
      ],
      "source": [
        "glove_6b50d = TokenEmbedding('glove.6b.50d')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99425a33",
      "metadata": {
        "origin_pos": 9,
        "id": "99425a33"
      },
      "source": [
        "Output the vocabulary size. The vocabulary contains 400000 words (tokens) and a special unknown token.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff0fe3b2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:40:18.101681Z",
          "iopub.status.busy": "2023-08-18T19:40:18.101067Z",
          "iopub.status.idle": "2023-08-18T19:40:18.107937Z",
          "shell.execute_reply": "2023-08-18T19:40:18.107092Z"
        },
        "origin_pos": 10,
        "tab": [
          "pytorch"
        ],
        "id": "ff0fe3b2",
        "outputId": "50ba5ed3-64e6-4761-9634-84d4b7dc9af4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "400001"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(glove_6b50d)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5839f20",
      "metadata": {
        "origin_pos": 11,
        "id": "c5839f20"
      },
      "source": [
        "We can get the index of a word in the vocabulary, and vice versa.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bee6bad4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:40:18.111818Z",
          "iopub.status.busy": "2023-08-18T19:40:18.111076Z",